{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Install and Import dependecies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install pytorch and requirements of yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from yolov5.models.yolo import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sheroz/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-5-19 Python-3.6.13 torch-1.8.1+cu111 CUDA:0 (GeForce GTX 960M, 4096MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.0 required by YOLOv5, but Python 3.6.13 is currently installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Make detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 'https://www.iii.org/sites/default/files/p_cars_highway_522785736.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m results\u001b[38;5;241m.\u001b[39mprint()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "results = model(img)\n",
    "results.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(np.squeeze(results.render()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[2.53091e+02, 3.55708e+02, 3.91220e+02, 4.61533e+02, 8.36293e-01, 2.00000e+00],\n",
       "         [4.16929e+00, 2.58595e+02, 1.63156e+02, 4.39846e+02, 8.19772e-01, 7.00000e+00],\n",
       "         [0.00000e+00, 3.54500e+02, 7.60661e+01, 4.78651e+02, 8.05330e-01, 2.00000e+00],\n",
       "         [0.00000e+00, 1.52520e+02, 6.10803e+01, 2.24085e+02, 7.75451e-01, 2.00000e+00],\n",
       "         [7.90970e+02, 2.76439e+02, 8.99539e+02, 3.60296e+02, 7.21499e-01, 2.00000e+00],\n",
       "         [6.26595e+01, 1.26641e+02, 1.37266e+02, 1.81878e+02, 7.11249e-01, 2.00000e+00],\n",
       "         [2.48977e+02, 5.18961e+01, 3.10397e+02, 9.90907e+01, 6.99802e-01, 2.00000e+00],\n",
       "         [6.11751e+02, 2.57864e+02, 7.05145e+02, 3.32778e+02, 6.72625e-01, 2.00000e+00],\n",
       "         [0.00000e+00, 1.09835e+02, 6.04686e+01, 1.62121e+02, 6.47430e-01, 2.00000e+00],\n",
       "         [4.78055e+02, 3.99501e+02, 6.13782e+02, 4.77083e+02, 6.36052e-01, 2.00000e+00],\n",
       "         [1.56516e+02, 5.27926e+01, 2.14368e+02, 9.91836e+01, 6.16537e-01, 2.00000e+00],\n",
       "         [4.39574e+01, 3.73954e-01, 1.11960e+02, 6.35450e+01, 6.13948e-01, 7.00000e+00],\n",
       "         [1.43324e+02, 2.24147e+02, 2.92752e+02, 3.64778e+02, 5.61499e-01, 7.00000e+00],\n",
       "         [3.56423e+02, 2.74245e+02, 4.62332e+02, 3.56458e+02, 5.55690e-01, 2.00000e+00],\n",
       "         [7.60247e+02, 3.88389e+02, 9.00936e+02, 4.77570e+02, 5.47037e-01, 2.00000e+00],\n",
       "         [7.58389e+02, 4.40204e+02, 8.83942e+02, 4.79803e+02, 5.30690e-01, 2.00000e+00],\n",
       "         [5.22099e+02, 3.42378e+02, 6.37750e+02, 4.33637e+02, 5.12662e-01, 2.00000e+00],\n",
       "         [1.14627e+02, 7.13922e+01, 1.58477e+02, 1.18388e+02, 4.94029e-01, 2.00000e+00],\n",
       "         [4.79317e+02, 1.53171e+02, 5.77916e+02, 2.39681e+02, 4.52659e-01, 7.00000e+00],\n",
       "         [2.80143e+02, 2.15594e+02, 3.61011e+02, 2.86217e+02, 3.92430e-01, 2.00000e+00],\n",
       "         [3.24233e+02, 1.11878e+02, 4.10367e+02, 2.15497e+02, 3.84237e-01, 7.00000e+00],\n",
       "         [6.61457e+02, 1.26876e+02, 7.43647e+02, 2.03253e+02, 3.79844e-01, 7.00000e+00],\n",
       "         [1.74483e+02, 1.70962e+01, 2.17502e+02, 5.01145e+01, 3.40299e-01, 2.00000e+00],\n",
       "         [8.32088e+02, 2.21857e+02, 9.13385e+02, 2.95019e+02, 3.25004e-01, 2.00000e+00],\n",
       "         [3.24768e+02, 2.34301e+01, 3.71336e+02, 6.01232e+01, 3.08970e-01, 2.00000e+00],\n",
       "         [4.34625e+02, 4.60632e+02, 5.44379e+02, 4.80000e+02, 3.02616e-01, 2.00000e+00],\n",
       "         [7.00570e+02, 8.40095e+01, 7.60420e+02, 1.33580e+02, 2.68254e-01, 2.00000e+00]], device='cuda:0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.xyxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Real-time detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('saved_models/cheetah_yolov5s.pt')  # load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'cheetah_vid.mpg4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Make detections \u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#results = model(frame)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLO\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\u001b[38;5;66;03m#np.squeeze(results.render()))\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     12\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Make detections \n",
    "    #results = model(frame)\n",
    "    \n",
    "    cv2.imshow('YOLO', frame)#np.squeeze(results.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Train from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
